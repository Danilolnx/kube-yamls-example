# This is an example of an OpenShift-Ansible host inventory that provides the
# minimum recommended configuration for production use. This includes 3 masters,
# two infra nodes, two compute nodes, and an haproxy load balancer to load
# balance traffic to the API servers. For a truly production environment you
# should use an external load balancing solution that itself is highly available.

[masters]
#ose3-master[336:338].test.example.com
sinvp-336.in.local openshift_node_group_name="node-config-master"
sinvp-337.in.local openshift_node_group_name="node-config-master"
sinvp-338.in.local openshift_node_group_name="node-config-master"

[etcd]
#ose3-master[1:3].test.example.com
sinvp-336.in.local openshift_node_group_name="node-config-master"
sinvp-337.in.local openshift_node_group_name="node-config-master"
sinvp-338.in.local openshift_node_group_name="node-config-master"

[infra]
# logging | metrics |alertmanager | prometheus 
sinvp-339.in.local openshift_node_group_name="node-config-infra"
sinvp-340.in.local openshift_node_group_name="node-config-infra"
sinvp-341.in.local openshift_node_group_name="node-config-infra"

[nfs]
#ose3-master1.test.example.com
sinvp-198.in.local

[lb]
#ose3-lb.test.example.com #routers 
sinvp-342.in.local 
sinvp-343.in.local

[nodes]
sinvp-336.in.local openshift_node_group_name="node-config-master"
sinvp-337.in.local openshift_node_group_name="node-config-master"
sinvp-338.in.local openshift_node_group_name="node-config-master"
sinvp-339.in.local openshift_node_group_name="node-config-infra"
sinvp-340.in.local openshift_node_group_name="node-config-infra"
sinvp-341.in.local openshift_node_group_name="node-config-infra"
sinvp-344.in.local openshift_node_group_name="node-config-compute"
sinvp-345.in.local openshift_node_group_name="node-config-compute"
sinvp-346.in.local openshift_node_group_name="node-config-compute"
sinvp-342.in.local
sinvp-343.in.local

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
nodes
etcd
lb
nfs

[OSEv3:vars]
###############################################################################
# Common/ Required configuration variables follow                             #
###############################################################################
# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_user=root


# If ansible_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
#ansible_become=yes

# Specify the deployment type. Valid values are origin and openshift-enterprise.
openshift_deployment_type=origin
#openshift_deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release="3.11"

# default subdomain to use for exposed routes, you should have wildcard dns
openshift_master_default_subdomain=apps.in.local

#Set cluster_hostname to point at your load balancer
openshift_master_cluster_hostname=master.in.local



###############################################################################
# Additional configuration variables follow                                   #
###############################################################################

# Debug level for all OpenShift components (Defaults to 2)
debug_level=2

# Specify an exact container image tag to install or configure.
# WARNING: This value will be used for all hosts in containerized environments, even those that have another version installed.
# This could potentially trigger an upgrade and downtime, so be careful with modifying this value after the cluster is set up.
#openshift_image_tag=v3.11.0

# Specify an exact rpm version to install or configure.
# WARNING: This value will be used for all hosts in RPM based environments, even those that have another version installed.
# This could potentially trigger an upgrade and downtime, so be careful with modifying this value after the cluster is set up.
#openshift_pkg_version=-3.11.0

# If using Atomic Host, you may specify system container image registry for the nodes:
system_images_registry="nexus.in.local"

# Skip upgrading Docker during an OpenShift upgrade, leaves the current Docker version alone.
docker_upgrade=False

# Specify a list of block devices to be formatted and mounted on the nodes
# during prerequisites.yml. For each hash, "device", "path", "filesystem" are
# required. To add devices only on certain classes of node, redefine
# container_runtime_extra_storage as a group var.
#container_runtime_extra_storage='[{"device":"/dev/vdc","path":"/var/lib/origin/openshift.local.volumes","filesystem":"xfs","options":"gquota"}]'

# Enable etcd debug logging, defaults to false
# etcd_debug=true
# Set etcd log levels by package
# etcd_log_package_levels="etcdserver=WARNING,security=DEBUG"
# Comma-separated list of etcd cipher suites
# etcd_cipher_suites="TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256"

# Upgrade Hooks
#
# Hooks are available to run custom tasks at various points during a cluster
# upgrade. Each hook should point to a file with Ansible tasks defined. Suggest using
# absolute paths, if not the path will be treated as relative to the file where the
# hook is actually used.
#
# Tasks to run before each master is upgraded.
# openshift_master_upgrade_pre_hook=/usr/share/custom/pre_master.yml
#
# Tasks to run to upgrade the master. These tasks run after the main openshift-ansible
# upgrade steps, but before we restart system/services.
# openshift_master_upgrade_hook=/usr/share/custom/master.yml
#
# Tasks to run after each master is upgraded and system/services have been restarted.
# openshift_master_upgrade_post_hook=/usr/share/custom/post_master.yml


# Cluster Image Source (registry) configuration
# openshift-enterprise default is 'registry.redhat.io/openshift3/ose-${component}:${version}'

# origin default is 'docker.io/openshift/origin-${component}:${version}'
oreg_url=nexus.in.local/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true

# Add insecure and blocked registries to global docker configuration
openshift_docker_insecure_registries=registry-docker.in.gov.br

oreg_auth_user=some_user
oreg_auth_password='my-pass'


# NOTE: oreg_url must be defined by the user for oreg_auth_* to have any affect.
# oreg_auth_pass should be generated from running docker login.



# If the image for etcd needs to be pulled from anywhere else than registry.redhat.io, e.g. in
# a disconnected and containerized installation, use osm_etcd_image to specify the image to use:
osm_etcd_image=registry-docker.in.gov.br/rhel7/etcd


# LDAP auth
openshift_master_identity_providers=[{'name': 'ldap.in.local', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['dn'], 'email': ['mail'], 'name': ['cn'], 'preferredUsername': ['uid']}, 'bindDN': '', 'bindPassword': '', 'insecure': 'false', 'url': 'ldap://ldap.example.com:389/ou=users,dc=example,dc=com?uid'}]



# Native high availability (default cluster method)
# If no lb group is defined, the installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.

openshift_master_cluster_hostname=master.in.local

# If an external load balancer is used public hostname should resolve to
# external load balancer address
#openshift_master_cluster_public_hostname=openshift-ansible.public.example.com

# Configure datastore layer encryption, the encryption key can be gererated
# with the following command: 'head -c 32 /dev/urandom | base64'
#openshift_encryption_config="{'kind': 'EncryptionConfig','apiVersion': 'v1','resources': [{'resources': ['secrets'],'providers': [{'aescbc': {'keys': [{'name': 'key1','secret': 'xxxxxxxx'}]}},{'identity': {}}]}]}"
#
# OpenShift Router Options
#
# An OpenShift router will be created during install if there are
# nodes present with labels matching the default router selector,
# "node-role.kubernetes.io/infra=true".
#

# Router selector (optional)
# Router will only be created if nodes matching this label are present.
# Default value: 'node-role.kubernetes.io/infra=true'

openshift_hosted_router_selector='lb=true'
openshift_router_selector='lb=true'

#
# Router replicas (optional)
# Unless specified, openshift-ansible will calculate the replica count
# based on the number of nodes matching the openshift router selector.
openshift_hosted_router_replicas=2

#
# Router extended route validation (optional)
# If enabled, openshift-ansible will configure the router to perform extended
# validation on routes before admitting them.
#openshift_hosted_router_extended_validation=true

#
# Router force subdomain (optional)
# A router path format to force on all routes used by this router
# (will ignore the route host value)
#openshift_hosted_router_force_subdomain='${name}-${namespace}.apps.in.local'


# Router certificate (optional)
# Provide local certificate paths which will be configured as the
# router's default certificate.
#openshift_hosted_router_certificate={"certfile": "/path/to/router.crt", "keyfile": "/path/to/router.key", "cafile": "/path/to/router-ca.crt"}


# OpenShift Registry Console Options
# Override the console image prefix:
# origin default is "cockpit/", enterprise default is "openshift3/"
#openshift_cockpit_deployer_prefix=registry.example.com/myrepo/
# origin default is "kubernetes", enterprise default is "registry-console"
#openshift_cockpit_deployer_basename=my-console
# Override image version, defaults to latest for origin, vX.Y product version for enterprise
#openshift_cockpit_deployer_version=1.4.1


# Metrics deployment
# See: https://docs.openshift.com/container-platform/latest/install_config/cluster_metrics.html
#
# By default metrics are not automatically deployed, set this to enable them
openshift_metrics_install_metrics=true

# metrics-server deployment
# By default, metrics-server is not automatically deployed, unless metrics is also
# deployed.  Deploying metrics-server is necessary to use the HorizontalPodAutoscaler.
# Set this to enable it.
openshift_metrics_server_install=true
openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra":"true"}

# Storage Options
# If openshift_metrics_storage_kind is unset then metrics will be stored
# in an EmptyDir volume and will be deleted when the cassandra pod terminates.
# Storage options A & B currently support only one cassandra pod which is
# generally enough for up to 1000 pods. Additional volumes can be created
# manually after the fact and metrics scaled per the docs.
#


#
# Option B - External NFS Host
# NFS volume must already exist with path "nfs_directory/_volume_name" on
# the storage_host. For example, the remote volume path using these
# options would be "nfs.example.com:/exports/metrics".  "exports" is
# is the name of the export served by the nfs server.  "metrics" is
# the name of a directory inside of "/exports".
openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteMany']
openshift_metrics_storage_host=sinvp-198.in.local
openshift_metrics_storage_nfs_directory=/exports-prd
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=100Gi
openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_metrics_cassandra_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_metrics_heapster_nodeselector={"node-role.kubernetes.io/infra":"true"}



# Option C - Dynamic -- If openshift supports dynamic volume provisioning for
# your cloud platform use this.
#openshift_metrics_storage_kind=dynamic
#
# Other Metrics Options -- Common items you may wish to reconfigure, for the complete
# list of options please see roles/openshift_metrics/README.md
#
# Override metricsPublicURL in the master config for cluster metrics
# Defaults to https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics
# Currently, you may only alter the hostname portion of the url, alterting the
# `/hawkular/metrics` path will break installation of metrics.
openshift_metrics_hawkular_hostname=hawkular-metrics.apps.in.local

# Configure the metrics component images # Note, these will be modified by oreg_url by default

#openshift_metrics_cassandra_image="docker.io/openshift/origin-metrics-cassandra:{{ openshift_image_tag }}"
#openshift_metrics_hawkular_agent_image="docker.io/openshift/origin-metrics-hawkular-openshift-agent:{{ openshift_image_tag }}"
#openshift_metrics_hawkular_metrics_image="docker.io/openshift/origin-metrics-hawkular-metrics:{{ openshift_image_tag }}"
#openshift_metrics_schema_installer_image="docker.io/openshift/origin-metrics-schema-installer:{{ openshift_image_tag }}"
#openshift_metrics_heapster_image="docker.io/openshift/origin-metrics-heapster:{{ openshift_image_tag }}"

# when openshift_deployment_type=='openshift-enterprise'
#openshift_metrics_cassandra_image="registry.redhat.io/openshift3/metrics-cassandra:{{ openshift_image_tag }}"
#openshift_metrics_hawkular_agent_image="registry.redhat.io/openshift3/metrics-hawkular-openshift-agent:{{ openshift_image_tag }}"
#openshift_metrics_hawkular_metrics_image="registry.redhat.io/openshift3/metrics-hawkular-metrics:{{ openshift_image_tag }}"
#openshift_metrics_schema_installer_image="registry.redhat.io/openshift3/metrics-schema-installer:{{ openshift_image_tag }}"
#openshift_metrics_heapster_image="registry.redhat.io/openshift3/metrics-heapster:{{ openshift_image_tag }}"
#
# StorageClass
# openshift_storageclass_name=gp2
# openshift_storageclass_parameters={'type': 'gp2', 'encrypted': 'false'}
# openshift_storageclass_mount_options=['dir_mode=0777', 'file_mode=0777']
# openshift_storageclass_reclaim_policy="Delete"
#
# PersistentLocalStorage
# If Persistent Local Storage is wanted, this boolean can be defined to True.
# This will create all necessary configuration to use persistent storage on nodes.
#openshift_persistentlocalstorage_enabled=False
#openshift_persistentlocalstorage_classes=[]
#openshift_persistentlocalstorage_path=/mnt/local-storage
#openshift_persistentlocalstorage_provisionner_image=quay.io/external_storage/local-volume-provisioner:v1.0.1

# Cluster monitoring
#
# Cluster monitoring is enabled by default, disable it by setting
 openshift_cluster_monitoring_operator_install=true
openshift_cluster_monitoring_operator_node_selector={"node-role.kubernetes.io/infra":"true"}

# Cluster monitoring configuration variables allow setting the amount of
# storage and storageclass requested through PersistentVolumeClaims.
openshift_cluster_monitoring_operator_prometheus_storage_capacity="50Gi"
openshift_cluster_monitoring_operator_alertmanager_storage_capacity="2Gi"
#openshift_cluster_monitoring_operator_prometheus_storage_class_name=""
#openshift_cluster_monitoring_operator_alertmanager_storage_class_name=""


# Logging deployment

# Currently logging deployment is disabled by default, enable it by setting this
openshift_logging_install_logging=true
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra":"true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra":"true"}
 
openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteMany']
openshift_logging_storage_host=sinvp-198.in.local
openshift_logging_storage_nfs_directory=/exports-prd
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=100Gi

# Other Logging Options -- Common items you may wish to reconfigure, for the complete
# list of options please see roles/openshift_logging/README.md
#
# Configure loggingPublicURL in the master config for aggregate logging, defaults
# to kibana.{{ openshift_master_default_subdomain }}
openshift_logging_kibana_hostname=kibana.apps.in.local
# Configure the number of elastic search nodes, unless you're using dynamic provisioning
# this value must be 1

openshift_logging_es_cluster_size=1

# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')

os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#
# WARNING : Do not pick subnets that overlap with the default Docker bridge subnet of
# 172.17.0.0/16.  Your installation will fail and/or your configuration change will
# cause the Pod SDN or Cluster SDN to fail.
#
# WORKAROUND : If you must use an overlapping subnet, you can configure a non conflicting
# docker0 CIDR range by adding '--bip=192.168.2.1/24' to DOCKER_NETWORK_OPTIONS
# environment variable located in /etc/sysconfig/docker-network.
# When upgrading or scaling up the following must match whats in your master config!
#  Inventory: master yaml field
#  osm_cluster_network_cidr: clusterNetworkCIDR
#  openshift_portal_net: serviceNetworkCIDR
# When installing osm_cluster_network_cidr and openshift_portal_net must be set.
# Sane examples are provided below.
#osm_cluster_network_cidr=10.128.0.0/14
#openshift_portal_net=172.30.0.0/16

# ExternalIPNetworkCIDRs controls what values are acceptable for the
# service external IP field. If empty, no externalIP may be set. It
# may contain a list of CIDRs which are checked for access. If a CIDR
# is prefixed with !, IPs in that CIDR will be rejected. Rejections
# will be applied first, then the IP checked against one of the
# allowed CIDRs. You should ensure this range does not overlap with
# your nodes, pods, or service CIDRs for security reasons.
#openshift_master_external_ip_network_cidrs=['0.0.0.0/0']

# IngressIPNetworkCIDR controls the range to assign ingress IPs from for
# services of type LoadBalancer on bare metal. If empty, ingress IPs will not
# be assigned. It may contain a single CIDR that will be allocated from. For
# security reasons, you should ensure that this range does not overlap with
# the CIDRs reserved for external IPs, nodes, pods, or services.
#openshift_master_ingress_ip_network_cidr=172.46.0.0/16

# Configure number of bits to allocate to each host's subnet e.g. 9
# would mean a /23 network on the host.
# When upgrading or scaling up the following must match whats in your master config!
#  Inventory: master yaml field
#  osm_host_subnet_length:  hostSubnetLength
# When installing osm_host_subnet_length must be set. A sane example is provided below.
#osm_host_subnet_length=9

# Configure master API and console ports.
#openshift_master_api_port=8443
#openshift_master_console_port=8443

# set exact RPM version (include - prefix)
#openshift_pkg_version=-3.11.0
# you may also specify version and release, ie:
#openshift_pkg_version=-3.11.0-0.126.0.git.0.9351aae.el7

# Configure custom ca certificate
#openshift_master_ca_certificate={'certfile': '/path/to/ca.crt', 'keyfile': '/path/to/ca.key'}

# NOTE: CA certificate will not be replaced with existing clusters.
# This option may only be specified when creating a new cluster or
# when redeploying cluster certificates with the redeploy-certificates
# playbook.
#
# If you would like openshift_master_named_certificates to be overwritten with
# the provided value, specify openshift_master_overwrite_named_certificates.




#openshift_master_overwrite_named_certificates=true

# Provide local certificate paths which will be deployed to masters
#openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "cafile": "/path/to/custom-ca1.crt"}]
#
# Detected names may be overridden by specifying the "names" key
#openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"], "cafile": "/path/to/custom-ca1.crt"}]
#
# Add a trusted CA to all pods, copies from the control host, may be multiple
# certs in one file
#openshift_additional_ca=/path/to/additional-ca.crt





# An authentication and encryption secret will be generated if secrets
# are not provided. If provided, openshift_master_session_auth_secrets
# and openshift_master_encryption_secrets must be equal length.
#
# Signing secrets, used to authenticate sessions using
# HMAC. Recommended to use secrets with 32 or 64 bytes.
#openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
#
# Encrypting secrets, used to encrypt sessions. Must be 16, 24, or 32
# characters long, to select AES-128, AES-192, or AES-256.
#openshift_master_session_encryption_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']

# configure how often node iptables rules are refreshed
#openshift_node_iptables_sync_period=5s

# Configure nodeIP in the node config
# This is needed in cases where node traffic is desired to go over an
# interface other than the default network interface.
#openshift_set_node_ip=True

#openshift_node_kubelet_args is deprecated, use node config edits instead

# Configure logrotate scripts
# See: https://github.com/nickhammond/ansible-logrotate
logrotate_scripts=[{"name": "syslog", "path": "/var/log/cron\n/var/log/maillog\n/var/log/messages\n/var/log/secure\n/var/log/spooler\n", "options": ["daily", "rotate 7", "compress", "sharedscripts", "missingok"], "scripts": {"postrotate": "/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true"}}]

# The OpenShift-Ansible installer will fail when it detects that the
# value of openshift_kubelet_name_override resolves to an IP address not bound to any local
# interfaces. This mis-configuration is problematic for any pod leveraging host
# networking and liveness or readiness probes.
# Setting this variable to false will override that check.

#openshift_hostname_check=true

# Define an additional dnsmasq.conf file to deploy to /etc/dnsmasq.d/openshift-ansible.conf
# This is useful for POC environments where DNS may not actually be available yet or to set
# options like 'strict-order' to alter dnsmasq configuration.
#openshift_node_dnsmasq_additional_config_file=/home/bob/ose-dnsmasq.conf

# Global Proxy Configuration
# These options configure HTTP_PROXY, HTTPS_PROXY, and NOPROXY environment
# variables for docker and master services.
#
# Hosts in the openshift_no_proxy list will NOT use any globally
# configured HTTP(S)_PROXYs. openshift_no_proxy accepts domains
# (.example.com), hosts (example.com), and IP addresses.
#openshift_http_proxy=http://USER:PASSWORD@IPADDR:PORT
#openshift_https_proxy=https://USER:PASSWORD@IPADDR:PORT
#openshift_no_proxy='.hosts.example.com,some-host.com'
#
# Most environments don't require a proxy between openshift masters, nodes, and
# etcd hosts. So automatically add those hostnames to the openshift_no_proxy list.
# If all of your hosts share a common domain you may wish to disable this and
# specify that domain above instead.
#
# For example, having hosts with FQDNs: m1.ex.com, n1.ex.com, and
# n2.ex.com, one would simply add '.ex.com' to the openshift_no_proxy
# variable (above) and set this value to False
#openshift_generate_no_proxy_hosts=True
#
# Specify an openshift_service_catalog image
# (defaults for origin and openshift-enterprise, repsectively)
openshift_service_catalog_image="nexus.in.local/openshift/origin-service-catalog:{{ openshift_image_tag }}""

# Configure usage of openshift_clock role.
openshift_clock_enabled=true

# OpenShift Per-Service Environment Variables
# Environment variables are added to /etc/sysconfig files for
# each OpenShift node.
# API and controllers environment variables are merged in single
# master environments.
#openshift_node_env_vars={"ENABLE_HTTP2": "true"}

####################################################################### Enable API service auditing
openshift_master_audit_config={"enabled": "true"}
#
# In case you want more advanced setup for the auditlog you can
# use this line.
# The directory in "auditFilePath" will be created if it's not
# exist

openshift_master_audit_config={"enabled": "true", "auditFilePath": "/var/lib/origin/openpaas-oscp-audit/openpaas-oscp-audit.log", "maximumFileRetentionDays": "14", "maximumFileSizeMegabytes": "500", "maximumRetainedFiles": "5"}

# Enable origin repos that point at Centos PAAS SIG, defaults to true, only used
# by openshift_deployment_type=origin
#openshift_enable_origin_repo=false

# Validity of the auto-generated OpenShift certificates in days.
# See also openshift_hosted_registry_cert_expire_days above.


openshift_ca_cert_expire_days=5475
openshift_node_cert_expire_days=5475
openshift_master_cert_expire_days=5475
etcd_ca_default_days=5475

# ServiceAccountConfig:LimitSecretRefences rejects pods that reference secrets their service accounts do not reference
# openshift_master_saconfig_limitsecretreferences=false

# Upgrade Control
#
# By default nodes are upgraded in a serial manner one at a time and all failures
# are fatal, one set of variables for normal nodes, one set of variables for
# nodes that are part of control plane as the number of hosts may be different
# in those two groups.
#openshift_upgrade_nodes_serial=1
#openshift_upgrade_nodes_max_fail_percentage=0
#openshift_upgrade_control_plane_nodes_serial=1
#openshift_upgrade_control_plane_nodes_max_fail_percentage=0
#
# You can specify the number of nodes to upgrade at once. We do not currently
# attempt to verify that you have capacity to drain this many nodes at once
# so please be careful when specifying these values. You should also verify that
# the expected number of nodes are all schedulable and ready before starting an
# upgrade. If it's not possible to drain the requested nodes the upgrade will
# stall indefinitely until the drain is successful.
#
# If you're upgrading more than one node at a time you can specify the maximum
# percentage of failure within the batch before the upgrade is aborted. Any
# nodes that do fail are ignored for the rest of the playbook run and you should
# take care to investigate the failure and return the node to service so that
# your cluster.
#
# The percentage must exceed the value, this would fail on two failures
# openshift_upgrade_nodes_serial=4 openshift_upgrade_nodes_max_fail_percentage=49
# where as this would not
# openshift_upgrade_nodes_serial=4 openshift_upgrade_nodes_max_fail_percentage=50
#
# A timeout to wait for nodes to drain pods can be specified to ensure that the
# upgrade continues even if nodes fail to drain pods in the allowed time. The
# default value of 0 will wait indefinitely allowing the admin to investigate
# the root cause and ensuring that disruption budgets are respected. If the
# a timeout of 0 is used there will also be one attempt to re-try draining the
# node. If a non zero timeout is specified there will be no attempt to retry.
#openshift_upgrade_nodes_drain_timeout=0
#
# Multiple data migrations take place and if they fail they will fail the upgrade
# You may wish to disable these or make them non fatal
#
# openshift_upgrade_pre_storage_migration_enabled=true
# openshift_upgrade_pre_storage_migration_fatal=true
# openshift_upgrade_post_storage_migration_enabled=true
# openshift_upgrade_post_storage_migration_fatal=false

# Firewall configuration
# You can open additional firewall ports by defining them as a list. of service
# names and ports/port ranges for either masters or nodes.
#openshift_master_open_ports=[{"service":"svc1","port":"11/tcp"}]
#openshift_node_open_ports=[{"service":"svc2","port":"12-13/tcp"},{"service":"svc3","port":"14/udp"}]

# Service port node range
#openshift_node_port_range=30000-32767

# Enable unsupported configurations, things that will yield a partially
# functioning cluster but would not be supported for production use
#openshift_enable_unsupported_configurations=false
